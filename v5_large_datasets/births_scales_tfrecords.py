"""Denoising Autoencoder imputes data
"""
import tensorflow as tf
import numpy as np

births_scales_files = tf.train.string_input_producer(["births_scales.csv"])

reader = tf.TextLineReader(skip_header_lines=1)
key, row = reader.read(births_scales_files)

# record_defaults = np.full((1, 238), [0])
record_defaults = [
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
    [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],
]

# Default values, in case of empty columns. Also specifies the type of the
# decoded result.
v000, v001, v002, v003, v004, v005, v006, v007, v008, v009, \
v010, v011, v012, v013, v014, v015, v016, v017, v018, v019, \
v020, v021, v022, v023, v024, v025, v026, v027, v028, v029, \
v030, v031, v032, v033, v034, v035, v036, v037, v038, v039, \
v040, v041, v042, v043, v044, v045, v046, v047, v048, v049, \
v050, v051, v052, v053, v054, v055, v056, v057, v058, v059, \
v060, v061, v062, v063, v064, v065, v066, v067, v068, v069, \
v070, v071, v072, v073, v074, v075, v076, v077, v078, v079, \
v080, v081, v082, v083, v084, v085, v086, v087, v088, v089, \
v090, v091, v092, v093, v094, v095, v096, v097, v098, v099, \
v100, v101, v102, v103, v104, v105, v106, v107, v108, v109, \
v110, v111, v112, v113, v114, v115, v116, v117, v118, v119, \
v120, v121, v122, v123, v124, v125, v126, v127, v128, v129, \
v130, v131, v132, v133, v134, v135, v136, v137, v138, v139, \
v140, v141, v142, v143, v144, v145, v146, v147, v148, v149, \
v150, v151, v152, v153, v154, v155, v156, v157, v158, v159, \
v160, v161, v162, v163, v164, v165, v166, v167, v168, v169, \
v170, v171, v172, v173, v174, v175, v176, v177, v178, v179, \
v180, v181, v182, v183, v184, v185, v186, v187, v188, v189, \
v190, v191, v192, v193, v194, v195, v196, v197, v198, v199, \
v200, v201, v202, v203, v204, v205, v206, v207, v208, v209, \
v210, v211, v212, v213, v214, v215, v216, v217, v218, v219, \
v220, v221, v222, v223, v224, v225, v226, v227, v228, v229, \
v230, v231, v232, v233, v234, v235, v236, v237 = tf.decode_csv(row, record_defaults=record_defaults)

values = tf.stack([
    v000, v001, v002, v003, v004, v005, v006, v007, v008, v009, \
    v010, v011, v012, v013, v014, v015, v016, v017, v018, v019, \
    v020, v021, v022, v023, v024, v025, v026, v027, v028, v029, \
    v030, v031, v032, v033, v034, v035, v036, v037, v038, v039, \
    v040, v041, v042, v043, v044, v045, v046, v047, v048, v049, \
    v050, v051, v052, v053, v054, v055, v056, v057, v058, v059, \
    v060, v061, v062, v063, v064, v065, v066, v067, v068, v069, \
    v070, v071, v072, v073, v074, v075, v076, v077, v078, v079, \
    v080, v081, v082, v083, v084, v085, v086, v087, v088, v089, \
    v090, v091, v092, v093, v094, v095, v096, v097, v098, v099, \
    v100, v101, v102, v103, v104, v105, v106, v107, v108, v109, \
    v110, v111, v112, v113, v114, v115, v116, v117, v118, v119, \
    v120, v121, v122, v123, v124, v125, v126, v127, v128, v129, \
    v130, v131, v132, v133, v134, v135, v136, v137, v138, v139, \
    v140, v141, v142, v143, v144, v145, v146, v147, v148, v149, \
    v150, v151, v152, v153, v154, v155, v156, v157, v158, v159, \
    v160, v161, v162, v163, v164, v165, v166, v167, v168, v169, \
    v170, v171, v172, v173, v174, v175, v176, v177, v178, v179, \
    v180, v181, v182, v183, v184, v185, v186, v187, v188, v189, \
    v190, v191, v192, v193, v194, v195, v196, v197, v198, v199, \
    v200, v201, v202, v203, v204, v205, v206, v207, v208, v209, \
    v210, v211, v212, v213, v214, v215, v216, v217, v218, v219, \
    v220, v221, v222, v223, v224, v225, v226, v227, v228, v229, \
    v230, v231, v232, v233, v234, v235, v236, v237,
])

writer = tf.python_io.TFRecordWriter("births_scales.tfrecords")

with tf.Session() as sess:
    # Start populating the filename queue.
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    for i in range(16724141):
        features = sess.run(values)
        example = tf.train.Example(
            features=tf.train.Features(
                feature={
                    'values': tf.train.Feature(
                        float_list=tf.train.FloatList(value=features)),
                }
            )
        )
        serialized_example = example.SerializeToString()
        writer.write(serialized_example)

    coord.request_stop()
    coord.join(threads)
